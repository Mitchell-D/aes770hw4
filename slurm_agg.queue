#!/bin/csh

### SLURM batch script
### (based on https://it.nsstc.uah.edu/itwiki/Slurm.queue.html)

### Email address
#SBATCH --mail-user=mtd0012@uah.edu

### Job name
#SBATCH -J valid_eos

### Partition (queue), select shared for GPU
### Optionally specify a GPU type: --gres=gpu:rtx5000:1 or --gres=gpu:a100:1
##SBATCH -p shared --gres=gpu:a100:1
##SBATCH -p shared --gres=gpu:rtx5000:2

#SBATCH -p standard

### TOTAL processors (number of tasks)
#SBATCH --ntasks 40

### total run time estimate (D-HH:MM)
#SBATCH -t 1-00:00

### memory (MB per CPU)
#SBATCH --mem-per-cpu=2G

### Mail to user on job done and fail
#SBATCH --mail-type=END,FAIL

### Ouput files
##SBATCH -o /rhome/mdodson/aes770hw4/data/slurm/slurm_agg_2017_aqua.out
##SBATCH -e /rhome/mdodson/aes770hw4/data/slurm/slurm_agg_2017_aqua.err

##SBATCH -o /rhome/mdodson/aes770hw4/data/slurm/slurm_agg_2021_terra.out
##SBATCH -e /rhome/mdodson/aes770hw4/data/slurm/slurm_agg_2021_terra.err

#SBATCH -o /rhome/mdodson/aes770hw4/data/slurm/slurm_agg.out
#SBATCH -e /rhome/mdodson/aes770hw4/data/slurm/slurm_agg.err


### GPU stuff
## module load cuda
### Set dynamic link loader path variable to include CUDA and bins from conda
## setenv LD_LIBRARY_PATH /common/pkgs/cuda/cuda-11.4/lib64
## setenv LD_LIBRARY_PATH ${LD_LIBRARY_PATH}:/common/pkgs/cuda/cuda-11.4/extras/CUPTI/lib64
### Need to include links from conda lib for tensorflow
## setenv LD_LIBRARY_PATH ${LD_LIBRARY_PATH}:/rhome/mdodson/.conda/envs/learn/lib
## echo $LD_LIBRARY_PATH

## Load my default settings (using aes conda library.)
source /rhome/mdodson/.bashrc.matrix

cd /rhome/mdodson/aes770hw4

#Run code
set runcmd = /rhome/mdodson/.conda/envs/aes/bin/python

##${runcmd} -u aggregate_ceres_modis.py
${runcmd} -u list_agg.py
